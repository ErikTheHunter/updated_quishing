{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ec70db",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-17T21:57:47.586771Z",
     "iopub.status.busy": "2025-07-17T21:57:47.586446Z",
     "iopub.status.idle": "2025-07-17T21:57:49.882143Z",
     "shell.execute_reply": "2025-07-17T21:57:49.881272Z"
    },
    "papermill": {
     "duration": 2.302946,
     "end_time": "2025-07-17T21:57:49.884103",
     "exception": false,
     "start_time": "2025-07-17T21:57:47.581157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import random\n",
    "import gc\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import tldextract\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c77271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T21:57:49.892373Z",
     "iopub.status.busy": "2025-07-17T21:57:49.891894Z",
     "iopub.status.idle": "2025-07-17T21:57:49.896444Z",
     "shell.execute_reply": "2025-07-17T21:57:49.895532Z"
    },
    "papermill": {
     "duration": 0.010607,
     "end_time": "2025-07-17T21:57:49.898300",
     "exception": false,
     "start_time": "2025-07-17T21:57:49.887693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "N_SAMPLES = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba48c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T21:57:49.906264Z",
     "iopub.status.busy": "2025-07-17T21:57:49.905916Z",
     "iopub.status.idle": "2025-07-17T21:57:53.155504Z",
     "shell.execute_reply": "2025-07-17T21:57:53.154533Z"
    },
    "papermill": {
     "duration": 3.255641,
     "end_time": "2025-07-17T21:57:53.157334",
     "exception": false,
     "start_time": "2025-07-17T21:57:49.901693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url label\n",
      "0                     https://pllsadosaod.pages.dev/   bad\n",
      "1                             https://bit.ly/4cLJHjK   bad\n",
      "2                       https://nongenqa.weebly.com/   bad\n",
      "3  https://docs.google.com/presentation/d/e/2PACX...   bad\n",
      "4                  http://202.4.110.130:35612/Mozi.m   bad\n",
      "label\n",
      "bad     40000\n",
      "good    40000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add subdomains to ~50% of URLs\n",
    "def add_subdomain(domain):\n",
    "    if random.random() < SUBDOMAIN_PROB:\n",
    "        subdomain = random.choice(BENIGN_SUBDOMAINS)\n",
    "        return f\"{subdomain}.{domain}\"\n",
    "    return domain\n",
    "\n",
    "# --- Path to datasets ---\n",
    "tranco_csv_path = '/datasets/phishing-dataset/top-1m.csv'\n",
    "urlhaus_csv_path = '/datasets/phishing-dataset/urlhaus.csv'\n",
    "phishtank_csv_path = '/datasets/phishing-dataset/phishtank.csv'\n",
    "BENIGN_SUBDOMAINS = ['www', 'mail', 'm', 'app', 'api', 'blog', 'news', 'support', 'drive', 'shop', 'login', 'account']\n",
    "SEED = 42\n",
    "SUBDOMAIN_PROB = 0.5  # 50% chance to add a subdomain\n",
    "\n",
    "# --- Load Good URLs from Tranco ---\n",
    "\n",
    "try:\n",
    "    # Read CSV directly\n",
    "    df_tranco = pd.read_csv(tranco_csv_path, names=['rank', 'domain'], header=None)\n",
    "    \n",
    "    # Create df_good with required transformations\n",
    "    df_good = df_tranco[['domain']].copy()\n",
    "\n",
    "    random.seed(SEED)  \n",
    "    df_good['domain'] = df_good['domain'].apply(add_subdomain)\n",
    "    df_good['url'] = 'http://' + df_good['domain']\n",
    "    \n",
    "    # Drop null values\n",
    "    df_good = df_good.dropna()\n",
    "    # drop duplicates\n",
    "    df_good = df_good[['url']].drop_duplicates()\n",
    "    \n",
    "    # Sample N records with shuffling\n",
    "    df_good = df_good.sample(n=N_SAMPLES, random_state=42)\n",
    "    df_good['label'] = 'good'\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Tranco CSV file not found at {tranco_csv_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Tranco file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- Load Bad URLs from URLhaus ---\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Read CSV with explicit header\n",
    "    with open(urlhaus_csv_path, 'r') as f:\n",
    "        lines = [line for line in f if not line.startswith('#') and line.strip()]\n",
    "       \n",
    "    df_urlhaus = pd.read_csv(io.StringIO(''.join(lines)), low_memory=False, header=0)\n",
    "    \n",
    "    # Select the 'url' column if it exists, otherwise fall back to index\n",
    "    if 'url' in df_urlhaus.columns:\n",
    "        df_urlhaus = df_urlhaus[['url']].dropna()\n",
    "    else:\n",
    "        print(\"Warning: 'url' column not found. Falling back to index-based selection.\")\n",
    "        url_column_index = 2  # Based on previous output\n",
    "        if url_column_index < len(df_urlhaus.columns):\n",
    "            df_urlhaus = df_urlhaus.iloc[:, [url_column_index]].rename(columns={df_urlhaus.columns[url_column_index]: 'url'}).dropna()\n",
    "        else:\n",
    "            raise ValueError(f\"URL column index {url_column_index} is out of range. Available columns: {df_urlhaus.columns.tolist()}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: URLhaus CSV file not found at {urlhaus_csv_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error reading URLhaus file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- Load Bad URLs from PhishTank ---\n",
    "\n",
    "try:\n",
    "    df_phishtank = pd.read_csv(phishtank_csv_path)\n",
    "    if 'url' in df_phishtank.columns:\n",
    "        df_phishtank = df_phishtank[['url']].dropna()\n",
    "    else:\n",
    "        raise ValueError(\"PhishTank CSV does not have a 'url' column.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: PhishTank CSV file not found at {phishtank_csv_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error reading PhishTank file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- Combine, shuffle, and select N bad URLs ---\n",
    "try:\n",
    "    df_bad = pd.concat([df_urlhaus, df_phishtank], ignore_index=True)\n",
    "    df_bad = df_bad.drop_duplicates().sample(n=N_SAMPLES, random_state=42)\n",
    "    df_bad['label'] = 'bad'\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error combining bad URLs: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# --- Free up memory ---\n",
    "del df_tranco, df_urlhaus, df_phishtank\n",
    "gc.collect()\n",
    "\n",
    "# --- Merge good and bad DataFrames ---\n",
    "try:\n",
    "    df_all = pd.concat([df_good, df_bad], ignore_index=True)\n",
    "    df_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle the merged DataFrame\n",
    "\n",
    "    # Print results\n",
    "    print(df_all.head())\n",
    "    print(df_all['label'].value_counts())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error merging DataFrames: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bb989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T21:57:53.165828Z",
     "iopub.status.busy": "2025-07-17T21:57:53.165512Z",
     "iopub.status.idle": "2025-07-17T21:57:53.186314Z",
     "shell.execute_reply": "2025-07-17T21:57:53.185250Z"
    },
    "papermill": {
     "duration": 0.027118,
     "end_time": "2025-07-17T21:57:53.188165",
     "exception": false,
     "start_time": "2025-07-17T21:57:53.161047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUSPICIOUS_KEYWORDS = [\n",
    "    'login', 'update', 'free', 'verify', 'secure', 'account', 'bank', 'confirm', 'password', 'signin', 'pay', 'payment'\n",
    "]\n",
    "URL_SHORTENERS = [\n",
    "    'bit.ly', 'goo.gl', 'tinyurl.com', 'ow.ly', 't.co', 'is.gd', 'buff.ly', 'adf.ly', 'bit.do', 'cutt.ly', 'shorte.st'\n",
    "]\n",
    "PHISHY_TLDS = ['.xyz', '.top', '.ru', '.tk', '.ml', '.ga', '.cf', '.gq']\n",
    "\n",
    "# --- Helper functions for each feature ---\n",
    "def url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def count_dots(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def count_subdomains(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    if ext.subdomain == '':\n",
    "        return 0\n",
    "    return len(ext.subdomain.split('.'))\n",
    "\n",
    "def has_ip_address(url):\n",
    "    ipv4 = re.search(r'://(\\d{1,3}\\.){3}\\d{1,3}([/:]|$)', url)\n",
    "    ipv6 = re.search(r'://\\[[0-9a-fA-F:]+\\]', url)\n",
    "    return int(bool(ipv4 or ipv6))\n",
    "\n",
    "def has_suspicious_keywords(url):\n",
    "    url_lower = url.lower()\n",
    "    return int(any(word in url_lower for word in SUSPICIOUS_KEYWORDS))\n",
    "\n",
    "def count_special_chars(url):\n",
    "    chars = '-@=_/?&%#'\n",
    "    return sum(url.count(c) for c in chars)\n",
    "\n",
    "def has_https(url):\n",
    "    return int(url.lower().startswith('https://'))\n",
    "\n",
    "def url_entropy(url):\n",
    "    prob = [float(url.count(c)) / len(url) for c in set(url)]\n",
    "    entropy = - sum([p * math.log2(p) for p in prob])\n",
    "    return entropy\n",
    "\n",
    "def get_tld(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return '.' + ext.suffix if ext.suffix else ''\n",
    "\n",
    "def tld_is_phishy(url):\n",
    "    tld = get_tld(url)\n",
    "    return int(tld in PHISHY_TLDS)\n",
    "\n",
    "def path_length(url):\n",
    "    parsed = urlparse(url)\n",
    "    return len(parsed.path)\n",
    "\n",
    "def path_level(url):\n",
    "    parsed = urlparse(url)\n",
    "    return len([p for p in parsed.path.split('/') if p])\n",
    "\n",
    "def uses_url_shortener(url):\n",
    "    netloc = urlparse(url).netloc.lower()\n",
    "    return int(any(shortener in netloc for shortener in URL_SHORTENERS))\n",
    "\n",
    "def has_homograph(url):\n",
    "    try:\n",
    "        url.encode('ascii')\n",
    "        return 0\n",
    "    except UnicodeEncodeError:\n",
    "        return 1\n",
    "\n",
    "def has_multiple_slash_after_domain(url):\n",
    "    match = re.search(r'^[a-z]+://[^/]+//', url)\n",
    "    return int(bool(match))\n",
    "\n",
    "def https_in_hostname(url):\n",
    "    netloc = urlparse(url).netloc.lower()\n",
    "    return int('https' in netloc)\n",
    "\n",
    "def count_numeric_chars(url):\n",
    "    return sum(c.isdigit() for c in url)\n",
    "\n",
    "def query_length(url):\n",
    "    parsed = urlparse(url)\n",
    "    return len(parsed.query)\n",
    "\n",
    "def query_component_count(url):\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.query == '':\n",
    "        return 0\n",
    "    return len(parsed.query.split('&'))\n",
    "\n",
    "def brand_in_subdomain_or_path(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    subdomain = ext.subdomain.lower()\n",
    "    path = urlparse(url).path.lower()\n",
    "    for brand in SUSPICIOUS_KEYWORDS:\n",
    "        if brand in subdomain or brand in path:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def unusual_subdomains(url):\n",
    "    return int(count_subdomains(url) > 2)\n",
    "\n",
    "def extract_lexical_features(df, url_col='url'):\n",
    "    df_feat = df.copy()\n",
    "    df_feat['url_length'] = df_feat[url_col].apply(url_length)\n",
    "    df_feat['num_dots'] = df_feat[url_col].apply(count_dots)\n",
    "    df_feat['num_subdomains'] = df_feat[url_col].apply(count_subdomains)\n",
    "    df_feat['has_ip'] = df_feat[url_col].apply(has_ip_address)\n",
    "    df_feat['has_suspicious_keywords'] = df_feat[url_col].apply(has_suspicious_keywords)\n",
    "    df_feat['special_char_count'] = df_feat[url_col].apply(count_special_chars)\n",
    "    df_feat['url_entropy'] = df_feat[url_col].apply(url_entropy)\n",
    "    df_feat['tld'] = df_feat[url_col].apply(get_tld)\n",
    "    df_feat['tld_is_phishy'] = df_feat[url_col].apply(tld_is_phishy)\n",
    "    df_feat['path_level'] = df_feat[url_col].apply(path_level)\n",
    "    df_feat['uses_shortener'] = df_feat[url_col].apply(uses_url_shortener)\n",
    "    df_feat['has_homograph'] = df_feat[url_col].apply(has_homograph)\n",
    "    df_feat['multiple_slash_after_domain'] = df_feat[url_col].apply(has_multiple_slash_after_domain)\n",
    "    df_feat['https_in_hostname'] = df_feat[url_col].apply(https_in_hostname)\n",
    "    df_feat['numeric_char_count'] = df_feat[url_col].apply(count_numeric_chars)\n",
    "    df_feat['query_length'] = df_feat[url_col].apply(query_length)\n",
    "    df_feat['query_component_count'] = df_feat[url_col].apply(query_component_count)\n",
    "    df_feat['brand_in_subdomain_or_path'] = df_feat[url_col].apply(brand_in_subdomain_or_path)\n",
    "    df_feat['unusual_subdomains'] = df_feat[url_col].apply(unusual_subdomains)\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b00da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T21:57:53.196018Z",
     "iopub.status.busy": "2025-07-17T21:57:53.195737Z",
     "iopub.status.idle": "2025-07-17T21:58:00.512848Z",
     "shell.execute_reply": "2025-07-17T21:58:00.511894Z"
    },
    "papermill": {
     "duration": 7.322985,
     "end_time": "2025-07-17T21:58:00.514580",
     "exception": false,
     "start_time": "2025-07-17T21:57:53.191595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = extract_lexical_features(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489df00a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T21:58:00.522415Z",
     "iopub.status.busy": "2025-07-17T21:58:00.522103Z",
     "iopub.status.idle": "2025-07-17T21:58:00.537326Z",
     "shell.execute_reply": "2025-07-17T21:58:00.536135Z"
    },
    "papermill": {
     "duration": 0.021038,
     "end_time": "2025-07-17T21:58:00.538916",
     "exception": false,
     "start_time": "2025-07-17T21:58:00.517878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url label  url_length  \\\n",
      "0                     https://pllsadosaod.pages.dev/   bad          30   \n",
      "1                             https://bit.ly/4cLJHjK   bad          22   \n",
      "2                       https://nongenqa.weebly.com/   bad          28   \n",
      "3  https://docs.google.com/presentation/d/e/2PACX...   bad         190   \n",
      "4                  http://202.4.110.130:35612/Mozi.m   bad          33   \n",
      "\n",
      "   num_dots  num_subdomains  has_ip  has_suspicious_keywords  \\\n",
      "0         2               1       0                        0   \n",
      "1         1               0       0                        0   \n",
      "2         2               1       0                        0   \n",
      "3         3               1       0                        0   \n",
      "4         4               0       1                        0   \n",
      "\n",
      "   special_char_count  url_entropy   tld  ...  path_level  uses_shortener  \\\n",
      "0                   3     3.672906  .dev  ...           0               0   \n",
      "1                   3     4.027169   .ly  ...           1               1   \n",
      "2                   3     4.083617  .com  ...           0               0   \n",
      "3                  17     5.535047  .com  ...           5               0   \n",
      "4                   3     3.945465        ...           1               0   \n",
      "\n",
      "   has_homograph  multiple_slash_after_domain  https_in_hostname  \\\n",
      "0              0                            0                  0   \n",
      "1              0                            0                  0   \n",
      "2              0                            0                  0   \n",
      "3              0                            0                  0   \n",
      "4              0                            0                  0   \n",
      "\n",
      "   numeric_char_count  query_length  query_component_count  \\\n",
      "0                   0             0                      0   \n",
      "1                   1             0                      0   \n",
      "2                   0             0                      0   \n",
      "3                  17            58                      4   \n",
      "4                  15             0                      0   \n",
      "\n",
      "   brand_in_subdomain_or_path  unusual_subdomains  \n",
      "0                           0                   0  \n",
      "1                           0                   0  \n",
      "2                           0                   0  \n",
      "3                           0                   0  \n",
      "4                           0                   0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee22ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T21:58:00.548353Z",
     "iopub.status.busy": "2025-07-17T21:58:00.547995Z",
     "iopub.status.idle": "2025-07-17T23:04:18.129963Z",
     "shell.execute_reply": "2025-07-17T23:04:18.128562Z"
    },
    "papermill": {
     "duration": 3977.592097,
     "end_time": "2025-07-17T23:04:18.135706",
     "exception": false,
     "start_time": "2025-07-17T21:58:00.543609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.968\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      7916\n",
      "           1       0.96      0.97      0.97      8084\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.97      0.97      0.97     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "[[7610  306]\n",
      " [ 206 7878]]\n",
      "Feature Importances for Random Forest:\n",
      "special_char_count             0.455936\n",
      "path_level                     0.166501\n",
      "url_length                     0.142702\n",
      "url_entropy                    0.134867\n",
      "numeric_char_count             0.045470\n",
      "num_dots                       0.015351\n",
      "num_subdomains                 0.011961\n",
      "query_component_count          0.005451\n",
      "has_ip                         0.005273\n",
      "tld_is_phishy                  0.004799\n",
      "has_suspicious_keywords        0.004139\n",
      "uses_shortener                 0.002886\n",
      "brand_in_subdomain_or_path     0.002418\n",
      "query_length                   0.002099\n",
      "unusual_subdomains             0.000102\n",
      "multiple_slash_after_domain    0.000027\n",
      "has_homograph                  0.000011\n",
      "https_in_hostname              0.000006\n",
      "dtype: float64\n",
      "CatBoost Accuracy: 0.97025\n",
      "Classification Report for CatBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      7916\n",
      "           1       0.96      0.98      0.97      8084\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.97      0.97      0.97     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "Confusion Matrix for CatBoost:\n",
      "[[7616  300]\n",
      " [ 176 7908]]\n",
      "Feature Importances for CatBoost:\n",
      "path_level                     33.510169\n",
      "special_char_count             18.892060\n",
      "url_length                     14.279206\n",
      "url_entropy                     8.694388\n",
      "numeric_char_count              6.913947\n",
      "num_subdomains                  4.682821\n",
      "num_dots                        4.258065\n",
      "brand_in_subdomain_or_path      2.006797\n",
      "has_suspicious_keywords         1.407958\n",
      "tld_is_phishy                   1.363882\n",
      "query_component_count           1.321980\n",
      "query_length                    1.315946\n",
      "uses_shortener                  1.013641\n",
      "has_ip                          0.333982\n",
      "has_homograph                   0.002908\n",
      "unusual_subdomains              0.001227\n",
      "https_in_hostname               0.000573\n",
      "multiple_slash_after_domain     0.000450\n",
      "dtype: float64\n",
      "CatBoost with TLD Accuracy: 0.9816875\n",
      "Classification Report for CatBoost with TLD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      7916\n",
      "           1       0.98      0.98      0.98      8084\n",
      "\n",
      "    accuracy                           0.98     16000\n",
      "   macro avg       0.98      0.98      0.98     16000\n",
      "weighted avg       0.98      0.98      0.98     16000\n",
      "\n",
      "Confusion Matrix for CatBoost with TLD:\n",
      "[[7764  152]\n",
      " [ 141 7943]]\n",
      "Best Parameters for Random Forest: {'max_depth': 21, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 241}\n",
      "Best Cross-Validation Accuracy for Random Forest: 0.9689375\n",
      "Tuned Random Forest Accuracy: 0.970625\n",
      "Classification Report for Tuned Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      7916\n",
      "           1       0.96      0.98      0.97      8084\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.97      0.97      0.97     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "Best Parameters for CatBoost: {'border_count': 219, 'depth': 6, 'iterations': 792, 'l2_leaf_reg': 2.2683180247728636, 'learning_rate': 0.16241742634326756}\n",
      "Best Cross-Validation Accuracy for CatBoost: 0.9694375000000001\n",
      "Tuned CatBoost Accuracy: 0.970625\n",
      "Classification Report for Tuned CatBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      7916\n",
      "           1       0.96      0.98      0.97      8084\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.97      0.97      0.97     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42690159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T23:04:18.145351Z",
     "iopub.status.busy": "2025-07-17T23:04:18.145001Z",
     "iopub.status.idle": "2025-07-17T23:04:18.151983Z",
     "shell.execute_reply": "2025-07-17T23:04:18.150657Z"
    },
    "papermill": {
     "duration": 0.014385,
     "end_time": "2025-07-17T23:04:18.153864",
     "exception": false,
     "start_time": "2025-07-17T23:04:18.139479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- Model Building ---\n",
    "\n",
    "# # Import necessary libraries\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from catboost import CatBoostClassifier\n",
    "# import pandas as pd\n",
    "\n",
    "# # Prepare features and labels\n",
    "# feature_cols = [col for col in df.columns if col not in ['url', 'label', 'tld']]\n",
    "# X = df[feature_cols]\n",
    "# y = df['label'].map({'good': 0, 'bad': 1})\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # --- Random Forest Model ---\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# y_pred_rf = rf.predict(X_test)\n",
    "# print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "# print(\"Classification Report for Random Forest:\")\n",
    "# print(classification_report(y_test, y_pred_rf))\n",
    "# print(\"Confusion Matrix for Random Forest:\")\n",
    "# print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# # Feature importance for Random Forest\n",
    "# importances_rf = rf.feature_importances_\n",
    "# feature_importance_rf = pd.Series(importances_rf, index=feature_cols).sort_values(ascending=False)\n",
    "# print(\"Feature Importances for Random Forest:\")\n",
    "# print(feature_importance_rf)\n",
    "\n",
    "# # --- CatBoost Model ---\n",
    "# cb = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42, verbose=0)\n",
    "# cb.fit(X_train, y_train)\n",
    "# y_pred_cb = cb.predict(X_test)\n",
    "# print(\"CatBoost Accuracy:\", accuracy_score(y_test, y_pred_cb))\n",
    "# print(\"Classification Report for CatBoost:\")\n",
    "# print(classification_report(y_test, y_pred_cb))\n",
    "# print(\"Confusion Matrix for CatBoost:\")\n",
    "# print(confusion_matrix(y_test, y_pred_cb))\n",
    "\n",
    "# # Feature importance for CatBoost\n",
    "# importances_cb = cb.get_feature_importance()\n",
    "# feature_importance_cb = pd.Series(importances_cb, index=feature_cols).sort_values(ascending=False)\n",
    "# print(\"Feature Importances for CatBoost:\")\n",
    "# print(feature_importance_cb)\n",
    "\n",
    "# # --- Optional: CatBoost with TLD as categorical feature ---\n",
    "# feature_cols_with_tld = [col for col in df.columns if col not in ['url', 'label']]\n",
    "# X_with_tld = df[feature_cols_with_tld]\n",
    "# cat_features = ['tld']\n",
    "# X_train_tld, X_test_tld, y_train_tld, y_test_tld = train_test_split(X_with_tld, y, test_size=0.2, random_state=42)\n",
    "# cb_with_tld = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42, cat_features=cat_features, verbose=0)\n",
    "# cb_with_tld.fit(X_train_tld, y_train_tld)\n",
    "# y_pred_cb_tld = cb_with_tld.predict(X_test_tld)\n",
    "# print(\"CatBoost with TLD Accuracy:\", accuracy_score(y_test_tld, y_pred_cb_tld))\n",
    "# print(\"Classification Report for CatBoost with TLD:\")\n",
    "# print(classification_report(y_test_tld, y_pred_cb_tld))\n",
    "# print(\"Confusion Matrix for CatBoost with TLD:\")\n",
    "# print(confusion_matrix(y_test_tld, y_pred_cb_tld))\n",
    "\n",
    "# # --- Hyperparameter Tuning for Random Forest ---\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [10, 20],\n",
    "#     'min_samples_split': [2, 5]\n",
    "# }\n",
    "# grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "# print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "# print(\"Best Cross-Validation Accuracy for Random Forest:\", grid_search_rf.best_score_)\n",
    "# best_rf = grid_search_rf.best_estimator_\n",
    "# y_pred_best_rf = best_rf.predict(X_test)\n",
    "# print(\"Tuned Random Forest Accuracy:\", accuracy_score(y_test, y_pred_best_rf))\n",
    "# print(\"Classification Report for Tuned Random Forest:\")\n",
    "# print(classification_report(y_test, y_pred_best_rf))\n",
    "\n",
    "# # --- Hyperparameter Tuning for CatBoost ---\n",
    "# param_grid_cb = {\n",
    "#     'iterations': [500, 1000],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'depth': [4, 6]\n",
    "# }\n",
    "# grid_search_cb = GridSearchCV(CatBoostClassifier(random_state=42, verbose=0), param_grid_cb, cv=5, scoring='accuracy')\n",
    "# grid_search_cb.fit(X_train, y_train)\n",
    "# print(\"Best Parameters for CatBoost:\", grid_search_cb.best_params_)\n",
    "# print(\"Best Cross-Validation Accuracy for CatBoost:\", grid_search_cb.best_score_)\n",
    "# best_cb = grid_search_cb.best_estimator_\n",
    "# y_pred_best_cb = best_cb.predict(X_test)\n",
    "# print(\"Tuned CatBoost Accuracy:\", accuracy_score(y_test, y_pred_best_cb))\n",
    "# print(\"Classification Report for Tuned CatBoost:\")\n",
    "# print(classification_report(y_test, y_pred_best_cb))\n",
    "\n",
    "# # for performance metrics consider accuracy, preceision, recall training time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab03509",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T23:04:18.163069Z",
     "iopub.status.busy": "2025-07-17T23:04:18.162742Z",
     "iopub.status.idle": "2025-07-17T23:04:18.459042Z",
     "shell.execute_reply": "2025-07-17T23:04:18.457851Z"
    },
    "papermill": {
     "duration": 0.303018,
     "end_time": "2025-07-17T23:04:18.460957",
     "exception": false,
     "start_time": "2025-07-17T23:04:18.157939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       url_length  num_dots  num_subdomains  has_ip  has_suspicious_keywords  \\\n",
      "47044          15         1               0       0                        0   \n",
      "44295          34         4               0       1                        0   \n",
      "74783          24         2               1       0                        0   \n",
      "70975          58         2               1       0                        0   \n",
      "46645          26         2               1       0                        0   \n",
      "...           ...       ...             ...     ...                      ...   \n",
      "67666          38         2               1       0                        0   \n",
      "51146          24         2               1       0                        0   \n",
      "42494          13         1               0       0                        0   \n",
      "52517          37         4               0       1                        0   \n",
      "7754           23         1               0       0                        0   \n",
      "\n",
      "       special_char_count  url_entropy  tld_is_phishy  path_level  \\\n",
      "47044                   2     3.323231              1           0   \n",
      "44295                   5     3.939829              0           1   \n",
      "74783                   2     3.855389              0           0   \n",
      "70975                   3     4.256363              0           0   \n",
      "46645                   3     3.796218              0           0   \n",
      "...                   ...          ...            ...         ...   \n",
      "67666                   4     4.359127              0           0   \n",
      "51146                   2     3.720176              0           0   \n",
      "42494                   2     3.392747              0           0   \n",
      "52517                   3     4.154871              0           1   \n",
      "7754                    2     3.762267              0           0   \n",
      "\n",
      "       uses_shortener  has_homograph  multiple_slash_after_domain  \\\n",
      "47044               0              0                            0   \n",
      "44295               0              0                            0   \n",
      "74783               0              0                            0   \n",
      "70975               1              0                            0   \n",
      "46645               0              0                            0   \n",
      "...               ...            ...                          ...   \n",
      "67666               0              0                            0   \n",
      "51146               0              0                            0   \n",
      "42494               0              0                            0   \n",
      "52517               0              0                            0   \n",
      "7754                0              0                            0   \n",
      "\n",
      "       https_in_hostname  numeric_char_count  query_length  \\\n",
      "47044                  0                   0             0   \n",
      "44295                  0                  11             6   \n",
      "74783                  0                   0             0   \n",
      "70975                  0                   4             0   \n",
      "46645                  0                   0             0   \n",
      "...                  ...                 ...           ...   \n",
      "67666                  0                   5             0   \n",
      "51146                  0                   0             0   \n",
      "42494                  0                   0             0   \n",
      "52517                  0                  15             0   \n",
      "7754                   0                   0             0   \n",
      "\n",
      "       query_component_count  brand_in_subdomain_or_path  unusual_subdomains  \n",
      "47044                      0                           0                   0  \n",
      "44295                      1                           0                   0  \n",
      "74783                      0                           0                   0  \n",
      "70975                      0                           0                   0  \n",
      "46645                      0                           0                   0  \n",
      "...                      ...                         ...                 ...  \n",
      "67666                      0                           0                   0  \n",
      "51146                      0                           0                   0  \n",
      "42494                      0                           0                   0  \n",
      "52517                      0                           0                   0  \n",
      "7754                       0                           0                   0  \n",
      "\n",
      "[16000 rows x 18 columns]\n",
      "[0 1 0 ... 0 1 0]\n",
      "[0 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "cc = rf.predict(X_test)\n",
    "print(cc)\n",
    "dd = cb.predict(X_test)\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85142246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Building ---\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "print('a')\n",
    "# Prepare features and labels\n",
    "feature_cols = [col for col in df.columns if col not in ['url', 'label', 'tld']]\n",
    "print('a')\n",
    "X = df[feature_cols]\n",
    "print('a')\n",
    "y = df['label'].map({'good': 0, 'bad': 1})\n",
    "\n",
    "print('a')\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('a')\n",
    "\n",
    "# --- Random Forest Model ---\n",
    "print('a')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "print('a')\n",
    "rf.fit(X_train, y_train)\n",
    "print('a')\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report for Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# --- Decision Tree Model ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print('a')\n",
    "dt = DecisionTreeClassifier(max_depth=20, min_samples_split=5,random_state=42)\n",
    "print('a')\n",
    "dt.fit(X_train, y_train)\n",
    "print('a')\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Classification Report for Decision Tree:\")\n",
    "print('a')\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print('a')\n",
    "print(\"Confusion Matrix for Decision Tree:\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# Feature importance for Decision Tree\n",
    "importances_dt = dt.feature_importances_\n",
    "feature_importance_dt = pd.Series(importances_dt, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"Feature Importances for Decision Tree:\")\n",
    "print(feature_importance_dt)\n",
    "\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "importances_rf = rf.feature_importances_\n",
    "feature_importance_rf = pd.Series(importances_rf, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"Feature Importances for Random Forest:\")\n",
    "print(feature_importance_rf)\n",
    "\n",
    "# --- CatBoost Model ---\n",
    "cb = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42, verbose=0)\n",
    "cb.fit(X_train, y_train)\n",
    "y_pred_cb = cb.predict(X_test)\n",
    "print(\"CatBoost Accuracy:\", accuracy_score(y_test, y_pred_cb))\n",
    "print(\"Classification Report for CatBoost:\")\n",
    "print(classification_report(y_test, y_pred_cb))\n",
    "print(\"Confusion Matrix for CatBoost:\")\n",
    "print(confusion_matrix(y_test, y_pred_cb))\n",
    "\n",
    "# Feature importance for CatBoost\n",
    "importances_cb = cb.get_feature_importance()\n",
    "feature_importance_cb = pd.Series(importances_cb, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"Feature Importances for CatBoost:\")\n",
    "print(feature_importance_cb)\n",
    "\n",
    "# --- Optional: CatBoost with TLD as categorical feature ---\n",
    "feature_cols_with_tld = [col for col in df.columns if col not in ['url', 'label']]\n",
    "X_with_tld = df[feature_cols_with_tld]\n",
    "cat_features = ['tld']\n",
    "X_train_tld, X_test_tld, y_train_tld, y_test_tld = train_test_split(X_with_tld, y, test_size=0.2, random_state=42)\n",
    "cb_with_tld = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42, cat_features=cat_features, verbose=0)\n",
    "cb_with_tld.fit(X_train_tld, y_train_tld)\n",
    "y_pred_cb_tld = cb_with_tld.predict(X_test_tld)\n",
    "print(\"CatBoost with TLD Accuracy:\", accuracy_score(y_test_tld, y_pred_cb_tld))\n",
    "print(\"Classification Report for CatBoost with TLD:\")\n",
    "print(classification_report(y_test_tld, y_pred_cb_tld))\n",
    "print(\"Confusion Matrix for CatBoost with TLD:\")\n",
    "print(confusion_matrix(y_test_tld, y_pred_cb_tld))\n",
    "\n",
    "# --- Hyperparameter Tuning for Random Forest using Randomized Search ---\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 300),          # Random integers between 50-299\n",
    "    'max_depth': randint(10, 30),              # Random integers between 10-29  \n",
    "    'min_samples_split': randint(2, 10),       # Random integers between 2-9\n",
    "    'min_samples_leaf': randint(1, 5),         # Additional parameter for more exploration\n",
    "    'max_features': ['sqrt', 'log2', None]     # Categorical choices\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42), \n",
    "    param_distributions=param_dist_rf, \n",
    "    n_iter=5,                    # Number of parameter combinations to try\n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1                     # Use all available cores\n",
    ")\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "print(\"Best Parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy for Random Forest:\", random_search_rf.best_score_)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "print(\"Tuned Random Forest Accuracy:\", accuracy_score(y_test, y_pred_best_rf))\n",
    "print(\"Classification Report for Tuned Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_best_rf))\n",
    "\n",
    "# --- Hyperparameter Tuning for CatBoost using Randomized Search ---\n",
    "param_dist_cb = {\n",
    "    'iterations': randint(300, 1500),          # Random integers between 300-1499\n",
    "    'learning_rate': uniform(0.01, 0.19),      # Uniform distribution between 0.01-0.2\n",
    "    'depth': randint(3, 10),                   # Random integers between 3-9\n",
    "    'l2_leaf_reg': uniform(1, 9),              # L2 regularization between 1-10\n",
    "    'border_count': randint(32, 256)           # Additional parameter for exploration\n",
    "}\n",
    "\n",
    "random_search_cb = RandomizedSearchCV(\n",
    "    CatBoostClassifier(random_state=42, verbose=0), \n",
    "    param_distributions=param_dist_cb, \n",
    "    n_iter=5,                    # Number of parameter combinations to try\n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1                     # Use all available cores\n",
    ")\n",
    "random_search_cb.fit(X_train, y_train)\n",
    "print(\"Best Parameters for CatBoost:\", random_search_cb.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy for CatBoost:\", random_search_cb.best_score_)\n",
    "best_cb = random_search_cb.best_estimator_\n",
    "y_pred_best_cb = best_cb.predict(X_test)\n",
    "print(\"Tuned CatBoost Accuracy:\", accuracy_score(y_test, y_pred_best_cb))\n",
    "print(\"Classification Report for Tuned CatBoost:\")\n",
    "print(classification_report(y_test, y_pred_best_cb))\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning for Decision Tree using Randomized Search \n",
    "\n",
    "param_dist_dt = {\n",
    "  'max_depth': randint(5, 30),               # Random integers between 5-29\n",
    "  'min_samples_split': randint(2, 20),       # Random integers between 2-19\n",
    "  'min_samples_leaf': randint(1, 10),        # Random integers between 1-9\n",
    "  'max_features': ['sqrt', 'log2', None],    # Categorical choices\n",
    "  'criterion': ['gini', 'entropy']           # Splitting criteria\n",
    "}\n",
    "\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "  DecisionTreeClassifier(random_state=42),\n",
    "  param_distributions=param_dist_dt,\n",
    "  n_iter=50,                    # Number of parameter combinations to try\n",
    "  cv=5,\n",
    "  scoring='accuracy',\n",
    "  random_state=42,\n",
    "  n_jobs=-1                     # Use all available cores\n",
    ")\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "print(\"Best Parameters for Decision Tree:\",\n",
    "random_search_dt.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy for Decision Tree:\",\n",
    "random_search_dt.best_score_)\n",
    "best_dt = random_search_dt.best_estimator_\n",
    "y_pred_best_dt = best_dt.predict(X_test)\n",
    "print(\"Tuned Decision Tree Accuracy:\", accuracy_score(y_test,\n",
    "y_pred_best_dt))\n",
    "print(\"Classification Report for Tuned Decision Tree:\")\n",
    "print(classification_report(y_test, y_pred_best_dt))\n",
    "\n",
    "# for performance metrics consider accuracy, precision, recall training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06326d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T23:04:18.470507Z",
     "iopub.status.busy": "2025-07-17T23:04:18.470157Z",
     "iopub.status.idle": "2025-07-17T23:04:19.004632Z",
     "shell.execute_reply": "2025-07-17T23:04:19.003466Z"
    },
    "papermill": {
     "duration": 0.541035,
     "end_time": "2025-07-17T23:04:19.006093",
     "exception": false,
     "start_time": "2025-07-17T23:04:18.465058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for http://www.google.com:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.14%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.06%)\n",
      "\n",
      "Predictions for http://google.com:\n",
      "Random Forest: good (Malicious Probability: 1.28%)\n",
      "CatBoost: good (Malicious Probability: 0.60%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.96%)\n",
      "\n",
      "Predictions for http://docs.google.com:\n",
      "Random Forest: good (Malicious Probability: 4.73%)\n",
      "CatBoost: good (Malicious Probability: 0.59%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.12%)\n",
      "\n",
      "Predictions for https://google.com:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.36%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.49%)\n",
      "\n",
      "Predictions for http://google.com:\n",
      "Random Forest: good (Malicious Probability: 1.28%)\n",
      "CatBoost: good (Malicious Probability: 0.60%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.96%)\n",
      "\n",
      "Predictions for http://www.google.com:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.14%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.06%)\n",
      "\n",
      "Predictions for http://eportal.sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.58%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.08%)\n",
      "\n",
      "Predictions for http://mail.sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.07%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.02%)\n",
      "\n",
      "Predictions for http://sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.04%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.03%)\n",
      "\n",
      "Predictions for http://www.sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.18%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.02%)\n",
      "\n",
      "Predictions for http://sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.04%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.03%)\n",
      "\n",
      "Predictions for https://sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.13%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.04%)\n",
      "\n",
      "Predictions for https://www.sec.gov.ng:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.08%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.03%)\n",
      "\n",
      "Predictions for http://fgacebook.com:\n",
      "Random Forest: good (Malicious Probability: 0.74%)\n",
      "CatBoost: good (Malicious Probability: 0.68%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.66%)\n",
      "\n",
      "Predictions for https://fr-relais-lockers.com/check/calcul.php:\n",
      "Random Forest: bad (Malicious Probability: 100.00%)\n",
      "CatBoost: bad (Malicious Probability: 100.00%)\n",
      "CatBoost with TLD: bad (Malicious Probability: 100.00%)\n",
      "\n",
      "Predictions for http://56b5d4xg6fwn6hfn45hfn56h.dyndns.org:\n",
      "Random Forest: bad (Malicious Probability: 99.00%)\n",
      "CatBoost: bad (Malicious Probability: 99.55%)\n",
      "CatBoost with TLD: bad (Malicious Probability: 97.24%)\n",
      "\n",
      "Predictions for https://dlkzduilddnduoio6578.cfolks.pl/ai/auth/log.php:\n",
      "Random Forest: bad (Malicious Probability: 100.00%)\n",
      "CatBoost: bad (Malicious Probability: 100.00%)\n",
      "CatBoost with TLD: bad (Malicious Probability: 100.00%)\n",
      "\n",
      "Predictions for http://tiktokv.com:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 0.21%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.26%)\n",
      "\n",
      "Predictions for http://e2ro.com:\n",
      "Random Forest: good (Malicious Probability: 0.01%)\n",
      "CatBoost: good (Malicious Probability: 0.12%)\n",
      "CatBoost with TLD: good (Malicious Probability: 0.03%)\n",
      "\n",
      "Predictions for http://westbrookhistoricalsociety.org:\n",
      "Random Forest: good (Malicious Probability: 0.00%)\n",
      "CatBoost: good (Malicious Probability: 1.29%)\n",
      "CatBoost with TLD: good (Malicious Probability: 3.84%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_url(url, rf_model, cb_model, cb_tld_model):\n",
    "    \"\"\"Predict if a URL is malicious using trained Random Forest and CatBoost models, returning probabilities as percentages.\"\"\"\n",
    "    try:\n",
    "        # Add protocol if missing (consistent with Tranco preprocessing)\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "        \n",
    "        # Extract features using existing logic\n",
    "        df_features = extract_lexical_features(pd.DataFrame({'url': [url]}))\n",
    "        \n",
    "        # Feature columns for Random Forest and CatBoost (without TLD)\n",
    "        feature_cols = [col for col in df_features.columns if col not in ['url', 'tld']]\n",
    "        X = df_features[feature_cols]\n",
    "        \n",
    "        # Feature columns for CatBoost with TLD\n",
    "        feature_cols_tld = [col for col in df_features.columns if col not in ['url']]\n",
    "        X_tld = df_features[feature_cols_tld]\n",
    "        \n",
    "        # Make predictions\n",
    "        rf_pred = rf_model.predict(X)[0]\n",
    "        rf_prob = rf_model.predict_proba(X)[0][1] * 100  # Convert to percentage\n",
    "        cb_pred = cb_model.predict(X)[0]\n",
    "        cb_prob = cb_model.predict_proba(X)[0][1] * 100\n",
    "        cb_tld_pred = cb_tld_model.predict(X_tld)[0]\n",
    "        cb_tld_prob = cb_tld_model.predict_proba(X_tld)[0][1] * 100\n",
    "        \n",
    "        # Map predictions to labels\n",
    "        label_map = {0: 'good', 1: 'bad'}\n",
    "        rf_label = label_map[rf_pred]\n",
    "        cb_label = label_map[cb_pred]\n",
    "        cb_tld_label = label_map[cb_tld_pred]\n",
    "        \n",
    "        # Return results\n",
    "        results = {\n",
    "            'url': url,\n",
    "            'random_forest': {\n",
    "                'prediction': rf_label,\n",
    "                'malicious_probability_percent': float(rf_prob)\n",
    "            },\n",
    "            'catboost': {\n",
    "                'prediction': cb_label,\n",
    "                'malicious_probability_percent': float(cb_prob)\n",
    "            },\n",
    "            'catboost_with_tld': {\n",
    "                'prediction': cb_tld_label,\n",
    "                'malicious_probability_percent': float(cb_tld_prob)\n",
    "            }\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting URL {url}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- we will use this url to test with first ---\n",
    "urls = [\n",
    "    'www.google.com',\n",
    "    'google.com',\n",
    "    'docs.google.com',\n",
    "    'https://google.com',\n",
    "    'http://google.com',\n",
    "    'http://www.google.com',\n",
    "    'eportal.sec.gov.ng',\n",
    "    'mail.sec.gov.ng',\n",
    "    'sec.gov.ng',\n",
    "    'www.sec.gov.ng',\n",
    "    'http://sec.gov.ng',\n",
    "    'https://sec.gov.ng',\n",
    "    'https://www.sec.gov.ng',\n",
    "    'fgacebook.com', \n",
    "    'https://fr-relais-lockers.com/check/calcul.php', \n",
    "    'http://56b5d4xg6fwn6hfn45hfn56h.dyndns.org', \n",
    "    'https://dlkzduilddnduoio6578.cfolks.pl/ai/auth/log.php',\n",
    "    'tiktokv.com',\n",
    "    'e2ro.com',\n",
    "    'westbrookhistoricalsociety.org'\n",
    "]\n",
    "for url in urls:\n",
    "    result = predict_url(url, rf, cb, cb_with_tld)\n",
    "    print(f\"\\nPredictions for {result['url']}:\")\n",
    "    print(f\"Random Forest: {result['random_forest']['prediction']} (Malicious Probability: {result['random_forest']['malicious_probability_percent']:.2f}%)\")\n",
    "    print(f\"CatBoost: {result['catboost']['prediction']} (Malicious Probability: {result['catboost']['malicious_probability_percent']:.2f}%)\")\n",
    "    print(f\"CatBoost with TLD: {result['catboost_with_tld']['prediction']} (Malicious Probability: {result['catboost_with_tld']['malicious_probability_percent']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a6c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T23:04:19.016328Z",
     "iopub.status.busy": "2025-07-17T23:04:19.015929Z",
     "iopub.status.idle": "2025-07-17T23:04:19.134310Z",
     "shell.execute_reply": "2025-07-17T23:04:19.133093Z"
    },
    "papermill": {
     "duration": 0.125204,
     "end_time": "2025-07-17T23:04:19.135848",
     "exception": false,
     "start_time": "2025-07-17T23:04:19.010644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved as rf_model.joblib, cb_model.cbm, cb_tld_model.cbm\n"
     ]
    }
   ],
   "source": [
    "# --- Save Trained Models ---\n",
    "import joblib\n",
    "\n",
    "# Save Random Forest model\n",
    "joblib.dump(rf, 'rf_model.joblib')\n",
    "\n",
    "# Save CatBoost models\n",
    "cb.save_model('cb_model.cbm')\n",
    "cb_with_tld.save_model('cb_tld_model.cbm')\n",
    "\n",
    "print(\"Models saved as rf_model.joblib, cb_model.cbm, cb_tld_model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8eabb9",
   "metadata": {
    "papermill": {
     "duration": 0.004026,
     "end_time": "2025-07-17T23:04:19.144398",
     "exception": false,
     "start_time": "2025-07-17T23:04:19.140372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1373456,
     "sourceId": 2280177,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4623455,
     "sourceId": 7877965,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7310560,
     "sourceId": 11649516,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7310759,
     "sourceId": 11649793,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "chapter5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3999.995203,
   "end_time": "2025-07-17T23:04:21.776512",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-17T21:57:41.781309",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
